---
sidebar_label: 'Module 4 Overview'
sidebar_position: 1
---

# Module 4: AI Integration and Capstone Project

This module focuses on the convergence of language models, perception, and robotic action for autonomous humanoid behavior. Students will learn to integrate LLMs, voice commands, and autonomous planning in humanoid robots.

## Overview

In this module, you will learn how to create intelligent humanoid robots that can understand and respond to voice commands, plan complex behaviors using large language models, and execute those plans as coordinated robotic actions. This module combines the power of natural language processing, cognitive planning, and robotic control to create truly autonomous systems.

## Learning Objectives

By the end of this module, you will be able to:

- Implement voice processing systems using OpenAI Whisper to convert natural language commands into actionable tasks
- Design cognitive planning systems using LLMs to translate high-level commands into ROS 2 action sequences
- Integrate voice, perception, navigation, and manipulation in a complete robot workflow
- Create an autonomous humanoid system that responds to voice commands with complex behaviors

## Prerequisites

Before starting this module, you should have:

- Basic Python programming skills
- Understanding of ROS 2 fundamentals (covered in Module 1)
- Knowledge of digital twin concepts (covered in Module 2)
- Familiarity with AI-robot brain concepts (covered in Module 3)

## Chapters

1. [Voice-to-Action with OpenAI Whisper](./voice-to-action-whisper.md) - Learn to convert natural language commands into actionable tasks
2. [Cognitive Planning with LLMs](./cognitive-planning-llms.md) - Explore how to translate high-level commands into ROS 2 action sequences
3. [Capstone Project: The Autonomous Humanoid](./capstone-autonomous-humanoid.md) - Integrate voice, perception, navigation, and manipulation in a complete robot workflow